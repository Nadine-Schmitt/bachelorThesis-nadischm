{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See **preprocessing.py** script.\n",
    "\n",
    "The preprocessing is done for each downloaded and extracted Wikipedia dump (i.e. for each language) twice in order to get a ``raw input corpus`` and an ``entity annotated corpus``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[argparse](https://docs.python.org/3/library/argparse.html), [base64](https://docs.python.org/3/library/base64.html), [io](https://docs.python.org/3/library/io.html), [json](https://docs.python.org/3/library/json.html), [os](https://docs.python.org/2/library/os.html), [pathlib](https://docs.python.org/3/library/pathlib.html), [pickle](https://docs.python.org/3/library/pickle.html), [nltk](https://www.nltk.org/api/nltk.tokenize.html), [re](https://docs.python.org/3/library/re.html) and [time](https://docs.python.org/3/library/time.html) are needed for this script to work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import base64\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import nltk\n",
    "import re\n",
    "import time\n",
    "#\n",
    "# our global variables\n",
    "acronymList = []    #list of acronyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The usage of the script can be seen with the default -h or --help flag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Windows [Version 10.0.17134.885]\r\n",
      "(c) 2018 Microsoft Corporation. Alle Rechte vorbehalten.\r\n",
      "\r\n",
      "(base) C:\\Users\\nadin\\Documents\\Bachelorarbeit\\Code>python preprocessing.py --help\n",
      "usage: preprocessing.py [-h] [-e] [-l] [-ger] [-it] [-fr] [-es]\r\n",
      "                        source target acListName\r\n",
      "\r\n",
      "Script for preprocessing wikipedia corpus\r\n",
      "\r\n",
      "positional arguments:\r\n",
      "  source           source file\r\n",
      "  target           target directory name to store corpus in\r\n",
      "  acListName       name of a file holding acronyms\r\n",
      "\r\n",
      "optional arguments:\r\n",
      "  -h, --help       show this help message and exit\r\n",
      "  -e, --entity     process entity text\r\n",
      "  -l, --lowercase  lower casing text\r\n",
      "  -ger, --german   preproccesing german wikipedia\r\n",
      "  -it, --italian   preproccesing italian wikipedia\r\n",
      "  -fr, --french    preproccesing french wikipedia\r\n",
      "  -es, --spanish   preproccesing spanish wikipedia\r\n",
      "\r\n",
      "(base) C:\\Users\\nadin\\Documents\\Bachelorarbeit\\Code>"
     ]
    }
   ],
   "source": [
    "%%cmd\n",
    "python preprocessing.py --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "readAcronymList-function reads the acronyms with dots into a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readAcronymList(fName):\n",
    "    acronymList.clear()\n",
    "    with open(fName, errors = 'ignore') as f:\n",
    "        for words in f:\n",
    "            testw = words.rstrip(\"\\n\")\n",
    "            if len(testw) > 0 :\n",
    "                acronymList.append(testw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When producing an entity annotated input corpus (-e is set), then each entity in the text is substituted with its ID. If there is for example the sentence _Yesterday Obama was in Paris_ with entity _(11,15) (Obama, Barack Obama)_, the character position from 11 to 15 (with characters _Obama_) is taken and substituted with the entity id Barack_Obama. \n",
    "\n",
    "Note that sometimes the returning char offset from the extracted Wikipedia dump is false and have to be corrected. For example in AA/wiki_00 there is such an entity with a wrong char offset: _(1631, 1637) archon archon_. As archon beginns at position 1634 in the text and without handling this _hon_ would be printed aditionally. Therefore the char offset for each entity is verified (if doVerify) and if it is wrong, it is corrected.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Wiki_Extractor(fileName, tt): \n",
    "    new_text = ''\n",
    "    with io.open(fileName, errors = 'ignore') as f:\n",
    "        for wiki_article in f.readlines():\n",
    "            # each line is a dict with keys ['id', 'url', 'title', 'text', 'internal_links']\n",
    "            wiki_article = json.loads(wiki_article)\n",
    "            \n",
    "            #getting raw text, when tt is False\n",
    "            if not tt:\n",
    "                #return wiki_article['text']\n",
    "                new_text += wiki_article['text']\n",
    "                #print(new_text)\n",
    "            #getting annotated text\n",
    "            else:\n",
    "                beginn = 0\n",
    "                doVerify = False\n",
    "                searchWindow = 10   # search up to 10 characters behind the char_offsets\n",
    "                textShift = 0       #how many characters are needed to shift the mention\n",
    "                \n",
    "                # The dictionary in 'internal_links' has begin and end char offsets as keys and \n",
    "                # then a tuple of mention (i.e. the link name) and entity (i.e. Wikipedia page \n",
    "                # name). Notice, f.ex. the line with \n",
    "                # \n",
    "                # (2317, 2328) ('Love Affair', 'Love Affair (1994 film)')\n",
    "                #\n",
    "                # where text 'Love Affair' was linking to the Wikipedia page 'Love Affair (1994 film)'\n",
    "                for (char_offsets, (mention, wiki_page_name)) in pickle.loads(base64.b64decode(wiki_article['internal_links'].encode('utf-8'))).items():\n",
    "                    #print(char_offsets, mention, wiki_page_name)\n",
    "                    entity_name = ''\n",
    "                    #the new text contains the whole text until the word beginns\n",
    "                    if wiki_article['text'][beginn] == \" \":\n",
    "                        new_text += wiki_article['text'][beginn:(char_offsets[0]+textShift)]\n",
    "                    else:\n",
    "                        new_text += \" \" + wiki_article['text'][beginn:(char_offsets[0]+textShift)]\n",
    "                    #create the word with underscores\n",
    "                    entity_list = wiki_page_name.split()\n",
    "                    if len(entity_list) >0: \n",
    "                        for index in range(len(entity_list)-1):\n",
    "                            entity_name += entity_list[index] + '_'\n",
    "                        entity_name += entity_list[-1]\n",
    "                    #new text gets the word with underscores\n",
    "                    if new_text[-1] != \" \":\n",
    "                        new_text += \" \"\n",
    "                        doVerify = True\n",
    "                    new_text += entity_name\n",
    "                    \n",
    "                    #verify if char offeset is correct\n",
    "                    if doVerify:\n",
    "                        sStart = char_offsets[0]\n",
    "                        sEnde = char_offsets[1] + searchWindow\n",
    "                        if sEnde > len(wiki_article['text']):\n",
    "                            sEnde = len(wiki_article['text']) -1\n",
    "                        iFound = wiki_article['text'][sStart:sEnde].find(mention)\n",
    "                        if iFound < 1:\n",
    "                            beginn = char_offsets[1] + textShift\n",
    "                        else:\n",
    "                            beginn = char_offsets[1] + iFound\n",
    "                            textShift = iFound\n",
    "                            searchWindow = 10 + iFound\n",
    "                    else:\n",
    "                        #the next step in the loop beginns at the index where the word ends \n",
    "                        beginn = char_offsets[1]                   \n",
    "    #return raw or entity annoated corpus\n",
    "    return new_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following function removes the title and category lines as well as unwanted newlines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim2article(inText):\n",
    "    #\n",
    "    # as python starts with 0 itLen as pointer in the text is 1 to large\n",
    "    itLen = len(inText) - 1\n",
    "    outText = \"\"\n",
    "    first = True\n",
    "    i = 0\n",
    "    #print(\"Textlen:\" + str(itLen))\n",
    "    \n",
    "    while i<itLen:\n",
    "        #print(\"next loop at position \" + str(i))\n",
    "        if first:\n",
    "            cPointer = inText.find(\"Category:\",i)\n",
    "            nlPointer = inText.find('\\n', i)\n",
    "            i = nlPointer + 1\n",
    "            first = False\n",
    "        else:\n",
    "            if inText[i+1] == \"\\n\": \n",
    "                i += 1\n",
    "            else:\n",
    "                nlPointer = inText.find('\\n',i)\n",
    "                if nlPointer != -1 and nlPointer < cPointer:\n",
    "                    outText += inText[i:nlPointer] + \" \"\n",
    "                    i = nlPointer +1\n",
    "                else:\n",
    "                    if nlPointer == -1:\n",
    "                        outText += inText[i:cPointer]\n",
    "                        i = itLen\n",
    "                    else:\n",
    "                        outText += inText[i:cPointer]\n",
    "                        cPointer = inText.find(\"Category:\",nlPointer)\n",
    "                        nlPointer = inText.find('\\n',nlPointer+1)\n",
    "                    \n",
    "                        while cPointer != -1 and nlPointer != -1 and not first:\n",
    "                            if nlPointer < cPointer:\n",
    "                                first = True\n",
    "                                i = nlPointer-1\n",
    "                            else:\n",
    "                                cPointer = inText.find(\"Category:\",cPointer+1)\n",
    "                                nlPointer = inText.find('\\n',nlPointer+1)\n",
    "                        if not first:\n",
    "                            i = itLen\n",
    "    return outText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "processText-function removes special characters, such as %, \\ and extra spaces from text. \n",
    "\n",
    "Important: German, Italian, Spanish and French language have special characters, e.g. the German Ä, which the WikiExtractor.py can not handle and replace it with Ã„. As the input corpora should contain Ä instead of Ã„ latter is replaced with Ä by using the [UTF-8 enconding cheatsheet](https://bueltge.de/wp-content/download/wk/utf-8\\_kodierungen.pdf):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processText(inText, german, italian, french, spanish):\n",
    "    # remove header...\n",
    "    outText = trim2article(inText)\n",
    "    #change to lowercase if argument l is given\n",
    "    #remove all the digits, special characters\n",
    "    outText = outText.replace('$' , '')\n",
    "    outText = outText.replace('%' , '')\n",
    "    \n",
    "    #german\n",
    "    if german:\n",
    "        outText =outText.replace('Ã¼','ü')\n",
    "        outText =outText.replace('Ã¶','ö')\n",
    "        outText =outText.replace('Ã¤','ä')\n",
    "        outText =outText.replace('ÃŸ','ß')\n",
    "        outText =outText.replace('â€™','’')\n",
    "        outText =outText.replace('Ã–','Ö')\n",
    "        outText =outText.replace('Ãœ','Ü')\n",
    "        outText =outText.replace('Ã„','Ä')\n",
    "        outText =outText.replace('â€“','–')  #langer Gedankenstrich\n",
    "        outText =outText.replace('Ã“','Ó') \n",
    "        \n",
    "    #italian\n",
    "    if italian:\n",
    "        outText =outText.replace('Ã¨','è')\n",
    "        outText =outText.replace('Ã¹','ù')\n",
    "        outText =outText.replace('Ã©','é')\n",
    "        outText =outText.replace('â€™','’')\n",
    "        outText =outText.replace('Ã','à')\n",
    "        outText =outText.replace('Ã¬','ì')\n",
    "        outText =outText.replace('Ã²','ò')\n",
    "        outText =outText.replace('Ã§','ç')\n",
    "        outText =outText.replace('Ã‰','É')\n",
    "        outText =outText.replace('Å”','À')\n",
    "        outText =outText.replace('ÄŒ','È')\n",
    "        outText =outText.replace('Äš','Ì')\n",
    "        outText =outText.replace('Ä›','ì')\n",
    "        outText =outText.replace('Ã?','Í')\n",
    "        outText =outText.replace('Ã-','í')\n",
    "        outText =outText.replace('ÄŽ','Ï')\n",
    "        outText =outText.replace('Ä?','ï')\n",
    "        outText =outText.replace('Å‡','Ò')\n",
    "        outText =outText.replace('Ã“','Ó') \n",
    "        outText =outText.replace('Ã³','ó')\n",
    "        outText =outText.replace('Å®','Ù')       \n",
    "        outText =outText.replace('Ãš','Ú') \n",
    "        outText =outText.replace('Ãº','ú')       \n",
    "                \n",
    "    #French\n",
    "    if french:\n",
    "        outText =outText.replace('Å”','À')\n",
    "        outText =outText.replace('Ã','à')\n",
    "        outText =outText.replace('Ã','Â')\n",
    "        outText =outText.replace('Ã¢','â')\n",
    "        outText =outText.replace('Ä†','Æ')\n",
    "        outText =outText.replace('Ä‡','æ')\n",
    "        outText =outText.replace('Ã‡','Ç')      \n",
    "        outText =outText.replace('Ã§','ç')\n",
    "        outText =outText.replace('ÄŒ','È')\n",
    "        outText =outText.replace('Ã¨','è')\n",
    "        outText =outText.replace('Ã‰','É')\n",
    "        outText =outText.replace('Ã©','é')\n",
    "        outText =outText.replace('Ä˜','Ê')\n",
    "        outText =outText.replace('Ä™','ê')\n",
    "        outText =outText.replace('Ã‹','Ë')\n",
    "        outText =outText.replace('Ã«','ë')\n",
    "        outText =outText.replace('ÃŽ','Î')\n",
    "        outText =outText.replace('Ã®','î')\n",
    "        outText =outText.replace('ÄŽ','Ï')\n",
    "        outText =outText.replace('Ä?','ï')\n",
    "        outText =outText.replace('Ã”','Ô')\n",
    "        outText =outText.replace('Ã´','ô')\n",
    "        outText =outText.replace('Åš','Œ')\n",
    "        outText =outText.replace('Å›','œ')\n",
    "        outText =outText.replace('Å®','Ù') \n",
    "        outText =outText.replace('Ã¹','ù')\n",
    "        outText =outText.replace('Å°','Û')\n",
    "        outText =outText.replace('Å±','û')\n",
    "        outText =outText.replace('Åº','Ÿ')\n",
    "        outText =outText.replace('Ë™','ÿ')\n",
    "    \n",
    "    #Spanish\n",
    "    if spanish:\n",
    "        outText =outText.replace('Ã?','Á')\n",
    "        outText =outText.replace('Ã¡','á')\n",
    "        outText =outText.replace('Ã§','ç')\n",
    "        outText =outText.replace('Ã‰','É')\n",
    "        outText =outText.replace('Ã©','é')\n",
    "        outText =outText.replace('Ã?','Í')\n",
    "        outText =outText.replace('Ã-','í')\n",
    "        outText =outText.replace('Åƒ','Ñ')\n",
    "        outText =outText.replace('Å„','ñ')\n",
    "        outText =outText.replace('Ã“','Ó') \n",
    "        outText =outText.replace('Ã³','ó')\n",
    "        outText =outText.replace('Ãš','Ú') \n",
    "        outText =outText.replace('Ãº','ú')\n",
    "        outText =outText.replace('Ã¼','ü')\n",
    "\n",
    "    outText = re.sub('[^a-zA-Z0-9ÄÖÜäöüßèùéàìòçÉÓÀÈÌìÍíÏïÒóÙÚúÂâÆæÇÊêËëÎîÔôŒœÛûŸÿÁáÑñ._–()’\\'-:&]', ' ', outText)\n",
    "    outText = outText.replace(',' , '')\n",
    "    \n",
    "    \n",
    "    #remove all extra spaces from the text\n",
    "    outText = re.sub(r'\\s+', ' ', outText)\n",
    "    return outText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following function checks if a sentence has an acronym with dot (is in AcronymList) as last word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isAcronym(sentence):\n",
    "    words = sentence.split()\n",
    "    test = (words[-1] in acronymList)\n",
    "    return test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes sentences are unwanted broken by [nltk](https://www.nltk.org/api/nltk.tokenize.html), since there can be a dot without the sentence ending, when there is an acronym (Inc. for example). \n",
    "\n",
    "If there is a dot and the next sentence starts with an uppercase letter, then it is a new sentence. In the other case, the next sentence is appended to the current sentence. However there are examples like _i.e. Germany_, where after a dot there is an uppercase letter, but the text should not be split. Therefore an AcronymList, which contains all acronyms with a dot of an given language, is used. If _i.e_  is for example in the list, then the unwanted broken sentences are combined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# combine sentences which where unwanted broken\n",
    "# sList : list of sentences\n",
    "#\n",
    "# return fixed list of sentences\n",
    "#\n",
    "def fixUnwantedBreak(sList):\n",
    "    # \n",
    "    outList = []\n",
    "    if len(sList) > 0:\n",
    "        eI = 1       #pointer in the input-list\n",
    "        oI = 0       #pointer in the out-list\n",
    "        outList.append(sList[0])\n",
    "        while eI < len(sList):\n",
    "            sentence = sList[eI].strip()\n",
    "            if sentence[0].isupper():\n",
    "                if isAcronym(outList[oI]):\n",
    "                    outList[oI] = outList[oI] + ' ' + sentence\n",
    "                else:\n",
    "                    outList.append(sentence)\n",
    "                    oI += 1\n",
    "            else:\n",
    "                outList[oI] = outList[oI] + ' ' + sentence\n",
    "            eI += 1\n",
    "    return outList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessed inputLists are saved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the finallist in the target file\n",
    "# filename - where to save the finallist\n",
    "# flist    - finallist which is saved\n",
    "def saveModelList(filename, flist):\n",
    "    print(\"saving the final list for training in \" + filename)\n",
    "    with open(filename, 'w', encoding=\"UTF-8\") as f:\n",
    "        #pickle.dump(flist, f)\n",
    "        for sentence in flist:\n",
    "            f.writelines(\"%s \" % w for w in sentence)\n",
    "            f.writelines(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following function does the preprocessing by using the functions defined above.\n",
    "\n",
    "With [nltk](https://www.nltk.org/api/nltk.tokenize.html) the text is split into a list of individual sentences.\n",
    "\n",
    "In order to learn a word or entity embedding from text with [Gensim's Word2Vec libary](https://radimrehurek.com/gensim/models/word2vec.html), the text is needed to be loaded and organized into sentences. [PathLineSentence](https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.PathLineSentence) is used. PathLineSentence processes all files in a directory in alphabetical order by filename and each file contains one sentence per line. \n",
    "\n",
    "That is why the preprocessed input text is stored in the inputList raw or inputList entity dictionary, in which each file contains a single sentence per line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the final inputlist\n",
    "def buildModelList(scr, texttype, lowercase, targetdir, german, italian, french, spanish):\n",
    "    #\n",
    "    #\n",
    "    #\n",
    "    sentence_list = []\n",
    "    subfolders = [f.path for f in os.scandir(scr) if f.is_dir() ]\n",
    "    for d in subfolders:\n",
    "        finallist = []\n",
    "        print(\"processing files in path \" + d)\n",
    "        files = [f.path for f in os.scandir(d) if not f.is_dir() ]\n",
    "        for f in files:\n",
    "            print(f)\n",
    "            # get text from article\n",
    "            original_output_text = Wiki_Extractor(f, texttype)\n",
    "            # prepare the text for further processing\n",
    "            output_text = processText(original_output_text, german, italian, french, spanish)\n",
    "            \n",
    "            # split text to individual sentences\n",
    "            sentences_list = nltk.sent_tokenize(output_text)\n",
    "            #sentences_list = output_text.split('.')  #split the raw text into a list of sentences\n",
    "            \n",
    "            sentences_list = fixUnwantedBreak(sentences_list)\n",
    "            \n",
    "            for sentence in sentences_list:\n",
    "                if lowercase:\n",
    "                    sentence = sentence.lower()\n",
    "                s = sentence.strip()\n",
    "                l = len(s)\n",
    "                if l > 0:\n",
    "                    if s[l-1] == \".\":\n",
    "                        s = s[0:l-1]\n",
    "                    #append only words with lenght > 0\n",
    "                    tmpList = []\n",
    "                    for w in s.split():\n",
    "                        if len(w) > 0 :\n",
    "                            tmpList.append(w)\n",
    "                    finallist.append(tmpList)\n",
    "        path = d.split(\"\\\\\")\n",
    "        partlistname = targetdir + \"\\\\\" + path[-1] + \"_list\"\n",
    "        print(\"Save partlist \" + partlistname)\n",
    "        saveModelList(partlistname, finallist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuartion and Main "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration\n",
    "parser = argparse.ArgumentParser(description='Script for preprocessing wikipedia corpus')\n",
    "parser.add_argument('source', type=str, help='source file')\n",
    "parser.add_argument('target', type=str, help='target directory name to store corpus in')\n",
    "parser.add_argument('acListName', type=str, help=\"name of a file holding acronyms\")\n",
    "parser.add_argument('-e', '--entity', action='store_true', help='process entity text')\n",
    "parser.add_argument('-l', '--lowercase', action='store_true', help='lower casing text')\n",
    "parser.add_argument('-ger', '--german', action='store_true', help='preproccesing german wikipedia')\n",
    "parser.add_argument('-it', '--italian', action='store_true', help='preproccesing italian wikipedia')\n",
    "parser.add_argument('-fr', '--french', action='store_true', help='preproccesing french wikipedia')\n",
    "parser.add_argument('-es', '--spanish', action='store_true', help='preproccesing spanish wikipedia')\n",
    "\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "#start recording time\n",
    "startTime = time.time()\n",
    "\n",
    "#read AcronymList\n",
    "readAcronymList(args.acListName)\n",
    "\n",
    "# if  no -e then args.entity= false and raw is done\n",
    "buildModelList(args.source, args.entity, args.lowercase, args.target, args.german, args.italian, args.french, args.spanish)\n",
    "\n",
    "#print building model time\n",
    "flTime = time.time()\n",
    "print(\"building model input takes \", flTime-startTime, \" seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Jupyter Notebook into py-script "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to script preprocessing.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
