{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequeny of words from KORE dataset of the raw input corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [CountWords_Raw.py](https://github.com/Nadine-Schmitt/bachelorThesis-nadischm/blob/master/Code/CountWords_Raw.py) script.\n",
    "\n",
    "For each entity from the KORE datatset the number of occurence in the raw input corpus is calculated and printed as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[pickle](https://docs.python.org/3/library/pickle.html), [argparse](https://docs.python.org/3/library/argparse.html), [os](https://docs.python.org/2/library/os.html) and [gensim](https://radimrehurek.com/gensim/) are needed for this script to work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import argparse\n",
    "import os\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The usage of the script can be seen with the default -h or --help flag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Windows [Version 10.0.17134.885]\r\n",
      "(c) 2018 Microsoft Corporation. Alle Rechte vorbehalten.\r\n",
      "\r\n",
      "(base) C:\\Users\\nadin\\Documents\\Bachelorarbeit\\Code>python CountWords_Raw.py --help\n",
      "usage: CountWords_Raw.py [-h] koreSource inputSource\r\n",
      "\r\n",
      "Script for translating wikipedia page names\r\n",
      "\r\n",
      "positional arguments:\r\n",
      "  koreSource   source folder with kore dataset\r\n",
      "  inputSource  source folder with inputlist raw\r\n",
      "\r\n",
      "optional arguments:\r\n",
      "  -h, --help   show this help message and exit\r\n",
      "\r\n",
      "(base) C:\\Users\\nadin\\Documents\\Bachelorarbeit\\Code>"
     ]
    }
   ],
   "source": [
    "%%cmd\n",
    "python CountWords_Raw.py --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read KORE dataset into list and split entities which multiple tokens( e.g. Apple Inc. into Apple and Inc.) in order to count for each word seperately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readKore(source):\n",
    "    list = [line.rstrip('\\n') for line in open(source, encoding=\"UTF-8\")]\n",
    "   \n",
    "    lenght = len(list)\n",
    "    #list[0] = 'Apple Inc.'\n",
    "    #list[195] = 'Golden Globe Award for Best Actor - Motion Picture Drama' #instead of Golden Globe Award for Best Actor â€“ Motion Picture Drama\n",
    "    #list[299]= 'Kärtsy Hatakka' #instead of KÃ¤rtsy Hatakka\n",
    "    #list[302]= 'Ragnarök' #instead of RagnarÃ¶k\n",
    "    #print(list)\n",
    "\n",
    "    for index in range(lenght):\n",
    "        if list[index][0] == \"\\t\":\n",
    "            list[index] = list[index][1:len(list[index])]\n",
    "    #raw: single words\n",
    "    listSingle = []\n",
    "    i=0\n",
    "    for e in list:\n",
    "        eNew = e.split(' ')\n",
    "        for ele in eNew:\n",
    "            listSingle.insert(i, ele)\n",
    "            i=i+1\n",
    "    \n",
    "    return listSingle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function reads the inputList raw file by file and for each word from the KORE dataset the number of occurence is calculated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadInputList(dirname, KoreList):\n",
    "    counterList = []\n",
    "    i = 0\n",
    "    while i < len(KoreList):\n",
    "        counterList.append(0)\n",
    "        i = i+1\n",
    "    loadSentence = []\n",
    "    filenames = [f.path for f in os.scandir(dirname) ]\n",
    "    for fname in filenames:\n",
    "        print(\"read sentences from \", fname)\n",
    "        loadSentence = []\n",
    "        with open(fname, 'r') as f:\n",
    "            line = f.readline()\n",
    "            while line:\n",
    "                words = line.split()\n",
    "                for w in words:\n",
    "                    loadSentence.append(w)\n",
    "                line = f.readline()\n",
    "            #print(loadSentence)\n",
    "            #count words\n",
    "            iterator = 0\n",
    "            for e in KoreList:\n",
    "                #print(e)\n",
    "                counterList[iterator] += loadSentence.count(e)\n",
    "                iterator = iterator +1\n",
    "                \n",
    "    return counterList               \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Script for translating wikipedia page names')\n",
    "parser.add_argument('koreSource', type=str, help='source folder with kore dataset')\n",
    "parser.add_argument('inputSource', type=str, help='source folder with inputlist raw')\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read Kore data\n",
    "KoreList = readKore(args.koreSource)\n",
    "#KoreList[0] ='Apple'\n",
    "for e in KoreList:\n",
    "    print(e) #print all words from Kore\n",
    "count = loadInputList(args.inputSource,KoreList)\n",
    "for e in count:\n",
    "    print(e) #print afterwords their occurence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Jupyter Notebook into py-script "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to script CountWords_Raw.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
