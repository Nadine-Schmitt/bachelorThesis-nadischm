{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualitative Examinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [QualitativeExaminations.py](https://github.com/Nadine-Schmitt/bachelorThesis-nadischm/blob/master/Code/QualitativeExaminations.py) script.\n",
    "\n",
    "This script train a word and entity model and do a qualitative examination afterwards.\n",
    "\n",
    "Therefore the following code reduces dimensionality of word embeddings with [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) or [t-SNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html). The two-dimensional representation of the words can be plotted by using [pythons matplotlib](https://matplotlib.org).\n",
    "\n",
    "In addition, the [most_similar() function](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar) of Gensim is used to show related words to a given word, e.g. cucumber.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[gensim](https://radimrehurek.com/gensim/), [matplotlib](https://matplotlib.org), [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html), [t-SNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html), [argparse](https://docs.python.org/3/library/argparse.html), [time](https://docs.python.org/3/library/time.html) and [multiprocessing](https://docs.python.org/3.4/library/multiprocessing.html?highlight=process) are needed for this script to work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import argparse\n",
    "from gensim.models import Word2Vec\n",
    "import time\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With following function the parameter setting is read. It returns a list of lists, in which each list is a parameter setting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read parameter settings and get list of lists in which each list is a parameter setting\n",
    "def readParaSetting(paraList):\n",
    "    finallist = []\n",
    "    lines = [line.rstrip('\\n') for line in open(paraList)]\n",
    "    #lines[-1] = lines[-1][0:len(lines[-1])-1]\n",
    "    for e in lines:\n",
    "        if len(e) > 0:\n",
    "            list =[]\n",
    "            listelements = e.split(' , ')\n",
    "            for i in listelements:\n",
    "                list.append(i)\n",
    "            finallist.append(list)\n",
    "    return finallist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot-function reduces the dimensionaltiy of given words (with [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) or [t-SNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)). Afterwards it draws the words into a plot. \n",
    "\n",
    "Required parameters are:\n",
    "- **saveName** name of the file to save the plot to\n",
    "- **model** which is the trained Word2Vec model\n",
    "- **wordList** a list of words that should be drawn\n",
    "- **pca** PCA is used if TRUE and t-SNE if FALSE\n",
    "- **byTurns** a different label align and colour is used for every second word\n",
    "- **connect** items which are next to each other in the word list are connected with arrows when this parameter is set\n",
    "- **xfrom** start of x axis \n",
    "- **xto** end of x axis \n",
    "- **yfrom** start of y axis \n",
    "- **yto** end of y axis \n",
    "- **heading** heading of the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(saveName, model, wordList, pca=False, byTurns=True, connect=True, xfrom=3, xto=3, yfrom=3, yto=3, heading=''):\n",
    "    \n",
    "    # get all embeddings for the given words in the list \n",
    "    embeddings = [model[word] for w in words]\n",
    "\n",
    "    #use pca to get two-dimensional presentation of the embeddings\n",
    "    if pca:\n",
    "        pca = PCA(n_components=2, whiten=True)\n",
    "        newEmbeddings = pca.fit(embeddings).transform(embeddings)\n",
    "    #use t-SNE\n",
    "    else:\n",
    "        tsn = TSNE(n_components=2, random_state=0)\n",
    "        newEmbeddings = tsne.fit_transform(embeddings)\n",
    "\n",
    "    # draw plot\n",
    "    plt.figure(figsize=(6,6))\n",
    "    if reducingMethod:\n",
    "        plt.axis([xfrom, xto, yfrom, yto])\n",
    "\n",
    "    first = True # colour byTurns\n",
    "    for point, w in zip(newEmbeddings , wordList):\n",
    "        # plot points\n",
    "        plt.scatter(point[0], point[1], c='r' if first else 'g')\n",
    "        # plot word byTurns\n",
    "        plt.annotate(\n",
    "            w, \n",
    "            xy = (point[0], point[1]),\n",
    "            xytext = (-7, -6) if first else (7, -6),\n",
    "            textcoords = 'offset points',\n",
    "            ha = 'right' if first else 'left',\n",
    "            va = 'bottom',\n",
    "            size = \"x-large\"\n",
    "        )\n",
    "        first = not first if byTurns else first\n",
    "\n",
    "    # arrows\n",
    "    if connect:\n",
    "        for i in range(0, len(wordList)-1, 2):\n",
    "            a = newEmbeddings[i][0] + 0.04\n",
    "            b = newEmbeddings[i][1]\n",
    "            c = newEmbeddings[i+1][0] - 0.04\n",
    "            d = newEmbeddings[i+1][1]\n",
    "            plt.arrow(a, b, c-a, d-b,\n",
    "                shape='full',\n",
    "                lw=0.1,\n",
    "                edgecolor='#bbbbbb',\n",
    "                facecolor='#bbbbbb',\n",
    "                length_includes_head=True,\n",
    "                head_width=0.08,\n",
    "                width=0.01\n",
    "            )\n",
    "\n",
    "    # heading\n",
    "    if heading:\n",
    "        plt.title(heading)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    #save diagram into saveName-file\n",
    "    plt.savefig(saveName, format ='png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start record time\n",
    "startTime = time.time()\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Script for training word embeddings')\n",
    "parser.add_argument('source', type=str, help='source folder with preprocessed input data')\n",
    "parser.add_argument('paraList', type=str, help='source folder of paraList')\n",
    "parser.add_argument('-t', '--threads', type=int, default=mp.cpu_count(), help='number of worker threads to train the model')\n",
    "args = parser.parse_args()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load input corpus for training and read parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load input corpus with PathLineSentences\n",
    "sentences = gensim.models.word2vec.PathLineSentences(args.source)\n",
    "\n",
    "#set parameter (read one parameter setting from a textfile)\n",
    "paraList = readParaSetting(args.paraList)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train model with word2vec\n",
    "#\n",
    "#for each parameter setting \n",
    "#note that here only one setting should be in list; readParaSetting is used to read the Parameter Setting from a text file)\n",
    "for e in paraList:\n",
    "    sgI = int(e[3])\n",
    "    cbowmeanI = int(e[6])\n",
    "    sizeI = int(e[0]) \n",
    "    windowI = int(e[1])\n",
    "    min_countI = int(e[2])\n",
    "    hsI = int(e[4])\n",
    "    negativeI = int(e[5]) \n",
    "        \n",
    "    model  = Word2Vec(sentences,\n",
    "                        size=sizeI,\n",
    "                        window=windowI,\n",
    "                        min_count=min_countI,\n",
    "                        workers= mp.cpu_count(),\n",
    "                        sg=sgI,\n",
    "                        hs=hsI,\n",
    "                        negative=negativeI,\n",
    "                        cbow_mean=cbowmeanI)\n",
    "\n",
    "#calculate training time and print it\n",
    "trainingTime = time.time()    \n",
    "print('model trained and it took:' , trainingTime  - startTime)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trained model have high-dimensional word vectors and with the plot-function a list of words can be plottet into a graph with 2 dimensions.\n",
    "\n",
    "In the following two classes of words are given, which are put by turns in a list of words. Note that the arrows parameter is set to TRUE in order to produce arrows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot food and their corresponding countries and save it into figures/food\n",
    "wordList = [\"sauerkraut\",\"Germany\",\"pizza\",\"Italy\",\"baguette\",\"France\",\"doner\",\"Turkey\"]\n",
    "plot('figures/currency', model, wordList, True, True, True, -3, 3, -2, 2, r'$PCA\\ Visualisierung:\\ Food$')\n",
    "\n",
    "# plot countries and their corresponding capitals and save it into figures/capitals\n",
    "wordList  = [\"Athens\",\"Greece\",\"Berlin\",\"Germany\",\"Paris\",\"France\",\"Bern\",\"Switzerland\",\"Vienna\",\"Austria\",\"Lisbon\",\"Portugal\",\"Moscow\",\"Russia\",\"Rome\",\"Italy\",\"Tokyo\",\"Japan\",\"London\",\"England\"]\n",
    "plot('figures/capitals', model, wordList, True, True, True, -3, 3, -2, 2.2, r'$PCA\\ Visualisierung:\\ Capitals$')\n",
    "\n",
    "# plot countries and their corresponding language and save it into figures/languages\n",
    "wordList = [\"Germany\",\"German\",\"Italy\",\"Italian\",\"France\",\"French\",\"Greece\",\"Greek\",\"Spain\",\"Spanish\",\"Sweden\",\"Swedish\"]\n",
    "plot('figures/language', model, wordList, True, True, True, -3, 3, -2, 1.7, r'$PCA\\ Visualisierung:\\ Language$')\n",
    "\n",
    "# plot countries and their corresponding currencies and save it into figures/currency\n",
    "wordList = [\"Switzerland\",\"franc\",\"Germany\",\"Euro\",\"England\",\"pound\",\"Japan\",\"yen\"]\n",
    "plot('figures/currency', model, wordList, True, True, True, -3, 3, -2, 2, r'$PCA\\ Visualisierung:\\ Currencies$')\n",
    "\n",
    "# plot countries and their corresponding head of government and save it into figures/government\n",
    "wordList = [\"Germany\",\"Merkel\",\"Russia\",\"Putin\",\"France\",\"Macron\",\"Austria\",\"Kurz\"]\n",
    "plot('figures/currency', model, wordList, True, True, True, -3, 3, -2, 2, r'$PCA\\ Visualisierung:\\ Head of government$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next 3 examples shows related words to a given word (tiger, cucumber and car) by using the [most_similar() function](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar) of Gensim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot related words to 'tiger' with similarity function from Gensim\n",
    "similar = model.most_similar(positive=[\"tiger\"], negative=[], topn=10)\n",
    "wordList = [sim[0] for sim in similar]\n",
    "plot('figures/tiger',model, wordList, True, False, False, -3, 2, -2, 2, r'$PCA\\ Visualisierung:\\ tiger$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot related words to 'cucumber' with similarity function from Gensim\n",
    "similar = model.most_similar(positive=[\"cucumber\"], negative=[], topn=10)\n",
    "wordList = [sim[0] for sim in similar]\n",
    "plot('figures/cucumber', model, wordList, True, False, False, -3, 2, -2, 2, r'$PCA\\ Visualisierung:\\ cucumber$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot related words to 'car' with similarity function from Gensim\n",
    "similar = model.most_similar(positive=[\"car\"], negative=[], topn=10)\n",
    "wordList = [sim[0] for sim in similar]\n",
    "plot('figures/cars', model, wordList, True, False, False, -3, 2, -2, 2, r'$PCA\\ Visualisierung:\\ car$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following the correct gender of a given name and the correct food category (fruit or vegetable) of a given food should be captured:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot name to get correct gender\n",
    "wordList = [\"Annika\",\"Anton\",\"Andrea\",\"Andreas\",\"Emily\",\"Charles\",\"Erica\",\"Florian\",\"Fiona\",\"Johannes\",\"Judith\",\"Lars\",\"Julia\",\"Leon\",\"Lisa\",\"Linus\",\"Lucia\",\"Mira\",\"Nicole\",\"Nico\",\"Simona\", \"Tom\"]\n",
    "plot('figures/gender', model, wordList, True, True, False, -3, 3, -1.5, 2.5, r'$PCA\\ Visualisierung:\\ Name\\ according to \\ gender$')\n",
    "\n",
    "#plot foods\n",
    "wordList = [\"banana\", \"cucumber\", \"orange\", \"tomato\", \"cherry\", \"garlic\", \"apple\", \"carrot\"]\n",
    "plot('figures/foodCategory', model, wordList, True, True, False, -3, 3, -1.5, 2.5, r'$PCA\\ Visualisierung:\\ Food category$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total run-time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endTime = time.time()\n",
    "print(\"total run-time\", endTime - startTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Jupyter Notebook into py-script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook QualitativeExaminations.ipynb to script\n",
      "[NbConvertApp] Writing 10947 bytes to QualitativeExaminations.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script QualitativeExaminations.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
